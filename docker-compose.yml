version: '3.8'

services:
  # 1. LLM 추론 엔진 서비스 (Ollama)
  ollama:
    # Ollama 공식 이미지 사용
    image: ollama/ollama
    # Ollama 포트를 호스트에 노출
    ports:
      - "11434:11434"
    # 모델 데이터를 로컬에 저장하여 컨테이너 재시작해도 유지되도록 설정
    volumes:
      - ollama_models:/root/.ollama
    # 컨테이너 시작 시 미리 Llama 3 모델을 다운로드 (선택사항)
    # command: /bin/bash -c "ollama serve & sleep 5 && ollama pull llama3"
    tty: true

  # 2. 백엔드 서비스 (FastAPI + RAG Engine)
  backend:
    # backend 폴더의 Dockerfile을 사용해 빌드
    build: ./backend
    # 포트 매핑: 호스트:컨테이너
    ports:
      - "8000:8000"
    # 백엔드가 Ollama 서비스가 준비되기를 기다리도록 설정
    depends_on:
      - ollama
    environment:
      # 백엔드 코드에서 Ollama를 'http://ollama:11434'로 접근하도록 설정해야 함
      OLLAMA_HOST: http://ollama:11434

  # 3. 프론트엔드 서비스 (Next.js)
  frontend:
    # frontend 폴더의 Dockerfile을 사용해 빌드
    build: ./frontend
    # 포트 매핑: 호스트:컨테이너
    ports:
      - "3000:3000"
    # 프론트엔드가 백엔드가 준비되기를 기다리도록 설정
    depends_on:
      - backend
    # 개발 중 코드 변경 시 바로 반영되도록 볼륨 설정 (개발용)
    # volumes:
    #   - ./frontend:/app

volumes:
  ollama_models: